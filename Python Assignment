{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ForShashank.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rej01DlIcrix",
        "outputId": "25a0e1d4-cbba-471b-cf7b-1749cc821dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar -xzf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "6yrJISWmejID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, ArrayType\n",
        "\n",
        "conf_obj = SparkConf()\n",
        "conf_obj.set(\"spark.app.name\", \"RevolveSolutionsAssignment\")\n",
        "conf_obj.set(\"spark.master\", \"local[*]\")\n",
        "\n",
        "spark = SparkSession.\\\n",
        "        builder.\\\n",
        "        config(conf=conf_obj).\\\n",
        "        getOrCreate()"
      ],
      "metadata": {
        "id": "T19oM5z8emK1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "np.random.seed(seed=42)\n",
        "\n",
        "class Customer(object):\n",
        "    def __init__(self, customer_id, loyalty_score):\n",
        "        self.customer_id = customer_id\n",
        "        self.value_score = loyalty_score\n",
        "\n",
        "\n",
        "def generate_customers(output_location_root, number_of_customers, return_data=True):\n",
        "    customers = []\n",
        "    with open(f'{output_location_root}/customers.csv', mode='w') as customers_file:\n",
        "        csv_writer = csv.writer(customers_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        csv_writer.writerow([\"customer_id\", \"loyalty_score\"])\n",
        "        for cid in range(1, number_of_customers + 1):\n",
        "            score = np.random.randint(low=1, high=11)\n",
        "            customer_id = f\"C{cid}\"\n",
        "            csv_writer.writerow([customer_id, score])\n",
        "            if return_data:\n",
        "                customers.append(Customer(customer_id, score))\n",
        "    return customers if return_data else None\n",
        "\n",
        "\n",
        "def generate_products(output_location_root, products_to_generate):\n",
        "    product_count_digits = int(math.log10(len(sum(products_to_generate.values(), []))) + 1)\n",
        "\n",
        "    product_id_lookup = {k: {} for k, v in products_to_generate.items()}\n",
        "    with open(f'{output_location_root}/products.csv', mode='w') as products_file:\n",
        "        csv_writer = csv.writer(products_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        csv_writer.writerow([\"product_id\", \"product_description\", \"product_category\"])\n",
        "        item_index = 1\n",
        "        for category in products_to_generate:\n",
        "            for item in products_to_generate[category]:\n",
        "                product_id = f\"P{str(item_index).zfill(product_count_digits)}\"\n",
        "                csv_writer.writerow([product_id, item, category])\n",
        "                product_id_lookup[category][item] = product_id\n",
        "                item_index += 1\n",
        "    return product_id_lookup\n",
        "\n",
        "\n",
        "def generate_transactions(output_location_root, customers, products, product_id_lookup, products_cats_frequency,\n",
        "                          start_datetime, end_datetime):\n",
        "    open_files = open_transaction_sinks(output_location_root, start_datetime, end_datetime)\n",
        "    product_cats_count = len(products.keys())\n",
        "    num_days = (end_datetime - start_datetime).days\n",
        "    all_days = [start_datetime + timedelta(days=d) for d in range(0, num_days + 1)]\n",
        "    customer_frequency_type = [int(num_days / 14), int(num_days / 10), int(num_days / 7), int(num_days / 5),\n",
        "                               int(num_days / 4), int(num_days / 3)]\n",
        "\n",
        "    for customer in customers:\n",
        "        num_transaction_days = random.choice(customer_frequency_type)\n",
        "        num_cats = random.randint(1, product_cats_count)\n",
        "        customer_transaction_days = sorted(random.sample(all_days, num_transaction_days))\n",
        "        cats = random.sample(products_cats_frequency, num_cats)\n",
        "        for day in customer_transaction_days:\n",
        "            transaction = {\n",
        "                \"customer_id\": customer.customer_id,\n",
        "                \"basket\": generate_basket(products, product_id_lookup, cats),\n",
        "                \"date_of_purchase\": str(day + timedelta(minutes=random.randint(168, 1439)))\n",
        "            }\n",
        "            open_files[to_canonical_date_str(day)].write(json.dumps(transaction) + \"\\n\")\n",
        "\n",
        "    for f in open_files.values():\n",
        "        f.close()\n",
        "\n",
        "\n",
        "def to_canonical_date_str(date_to_transform):\n",
        "    return date_to_transform.strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "def open_transaction_sinks(output_location_root, start_datetime, end_datetime):\n",
        "    root_transactions_dir = f\"{output_location_root}/transactions/\"\n",
        "    open_files = {}\n",
        "    days_to_generate = (end_datetime - start_datetime).days\n",
        "    for next_day_offset in range(0, days_to_generate + 1):\n",
        "        next_day = to_canonical_date_str(start_datetime + timedelta(days=next_day_offset))\n",
        "        day_directory = f\"{root_transactions_dir}/d={next_day}\"\n",
        "        os.makedirs(day_directory, exist_ok=True)\n",
        "        open_files[next_day] = open(f\"{day_directory}/transactions.json\", mode='w')\n",
        "    return open_files\n",
        "\n",
        "\n",
        "def generate_basket(products, product_id_lookup, cats):\n",
        "    num_items_in_basket = random.randint(1, 3)\n",
        "    basket = []\n",
        "    product_category = random.choice(cats)\n",
        "    for item in [random.choice(products[product_category]) for _ in range(0, num_items_in_basket)]:\n",
        "        product_id = product_id_lookup[product_category][item]\n",
        "        basket.append({\n",
        "            \"product_id\": product_id,\n",
        "            \"price\": random.randint(1, 2000)\n",
        "        })\n",
        "    return basket\n",
        "\n",
        "\n",
        "products_data = {\n",
        "    \"house\": [\"detergent\", \"kitchen roll\", \"bin liners\", \"shower gel\", \"scented candles\", \"fabric softener\",\n",
        "                \"cling film\", \"aluminium foil\", \"toilet paper\", \"kitchen knife\", \"dishwasher tablets\", \"ice pack\"],\n",
        "    \"clothes\": [\"men's dark green trousers\", \"women's shoes\", \"jumper\", \"men's belt\", \"women's black socks\",\n",
        "                \"men's striped socks\", \"men's trainers\", \"women's blouse\", \"women's red dress\"],\n",
        "    \"fruit_veg\": [\"avocado\", \"cherries\", \"scotch bonnets\", \"peppers\", \"broccoli\", \"potatoes\", \"grapes\",\n",
        "                    \"easy peeler\", \"mango\", \"lemon grass\", \"onions\", \"apples\", \"raspberries\"],\n",
        "    \"sweets\": [\"carrot cake\", \"salted caramel dark chocolate\", \"gummy bears\", \"kombucha\", \"ice cream\", \"irn bru\"],\n",
        "    \"food\": [\"steak\", \"chicken\", \"mince beef\", \"milk\", \"hummus\", \"activated charcoal croissant\", \"whole chicken\",\n",
        "                \"tuna\", \"smoked salmon\", \"camembert\", \"pizza\", \"oats\", \"peanut butter\", \"almond milk\", \"lentil soup\",\n",
        "                \"greek yoghurt\", \"parmesan\", \"coconut water\", \"chicken stock\",  \"water\"],\n",
        "    \"bws\": [\"red wine\", \"gin\", \"cognac\", \"cigarettes\"]\n",
        "}\n",
        "products_cats_frequency = [\"house\"]*15 + [\"clothes\"]*5 + [\"fruit_veg\"]*25 + [\"sweets\"] * 20 + [\"food\"] * 25 + \\\n",
        "                            [\"bws\"] * 10\n",
        "\n",
        "gen_id = \"starter\"\n",
        "output_location = f\"/content/input_data/{gen_id}\"\n",
        "os.makedirs(output_location, exist_ok=True)\n",
        "\n",
        "gen_customers = generate_customers(output_location, 137)\n",
        "product_id_lookup = generate_products(output_location, products_data)\n",
        "\n",
        "start_date = datetime(2018, 12, 1, 0, 0, 0)\n",
        "end_date = datetime(2019, 3, 1, 23, 59, 59)\n",
        "generate_transactions(output_location, gen_customers, products_data, product_id_lookup, products_cats_frequency,\n",
        "                        start_date, end_date)"
      ],
      "metadata": {
        "id": "kY7yNRiOeyDX"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df = spark.read.csv(\"/content/input_data/starter/customers.csv\", header=True, multiLine=True)\n",
        "products_df = spark.read.csv(\"/content/input_data/starter/products.csv\", header=True, multiLine=True)\n",
        "transaction_schema = StructType([\n",
        "        StructField(\"customer_id\", StringType(), True),\n",
        "        StructField(\"basket\", ArrayType(StructType([\n",
        "                StructField(\"product_id\", StringType(), True),\n",
        "                StructField(\"price\", IntegerType(), True)\n",
        "        ]))),\n",
        "        StructField(\"date_of_purchase\", StringType(), True)\n",
        "])\n",
        "transactions_raw_df = spark.read.text(\"/content/input_data/starter/transactions/d*/*\")\n",
        "transactions_df = transactions_raw_df.select(\n",
        "    F.from_json(F.col(\"value\"), transaction_schema).alias(\"map_cols\")\n",
        ").select(\n",
        "    \"map_cols.customer_id\",\n",
        "    \"map_cols.basket\",\n",
        "    \"map_cols.date_of_purchase\"\n",
        ")"
      ],
      "metadata": {
        "id": "l2kog6ltfd9P"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactionsdf1 = transactions_df.select(\n",
        "\t\"customer_id\",\n",
        "\t\"date_of_purchase\",\n",
        "\tF.explode(\"basket\").alias(\"basket_exp\")\n",
        ")\n",
        "transactionsdf2 = transactionsdf1.select(\n",
        "\t\"customer_id\",\n",
        "\t\"date_of_purchase\",\n",
        "\tF.col(\"basket_exp.product_id\").alias(\"product_id\"),\n",
        "\tF.col(\"basket_exp.price\").alias(\"price\")\n",
        ")\n",
        "\n",
        "trans_prod_df = transactionsdf2.alias(\"tdf\").join(\n",
        "\tproducts_df.alias(\"pdf\"),\n",
        "\ttransactionsdf2[\"product_id\"] == products_df[\"product_id\"],\n",
        "\t\"left\"\n",
        ").select(\n",
        "\t\"tdf.customer_id\",\n",
        "    \"tdf.product_id\",\n",
        "\t\"pdf.product_category\",\n",
        "    F.to_date(F.substring(\"date_of_purchase\",1,10), \"yyyy-MM-dd\").alias(\"date_of_purchase\")\n",
        ")\n",
        "\n",
        "\n",
        "rolled_up_df = trans_prod_df.groupby(\n",
        "    \"date_of_purchase\",\n",
        "    \"customer_id\",\n",
        "    \"product_category\",\n",
        "    \"product_id\"\n",
        ").count().orderBy(\n",
        "    \"date_of_purchase\",\n",
        "    \"customer_id\",\n",
        "    \"product_category\",\n",
        "    \"product_id\"\n",
        ")\n",
        "\n",
        "output_df = rolled_up_df.join(\n",
        "\tcustomers_df,\n",
        "\trolled_up_df[\"customer_id\"] == customers_df[\"customer_id\"],\n",
        "\t\"left\"\n",
        ").select(\n",
        "    \"date_of_purchase\",\n",
        "    rolled_up_df[\"customer_id\"],\n",
        "    customers_df[\"loyalty_score\"],\n",
        "    \"product_category\",\n",
        "    \"product_id\",\n",
        "    F.col(\"count\").alias(\"purchase_count\")\n",
        ")\n",
        "\n",
        "\n",
        "(output_df\n",
        "    .coalesce(1)\n",
        "    .write\n",
        "    .partitionBy(\"date_of_purchase\")\n",
        "    .mode(\"overwrite\")\n",
        "    .json(\"/content/output_data\"))"
      ],
      "metadata": {
        "id": "_29_6I6hg0m8"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r input_data"
      ],
      "metadata": {
        "id": "_aZcdOJU5Gsf"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r inputs_data_generator"
      ],
      "metadata": {
        "id": "7VsAT7fq6VdU"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r output_data_1"
      ],
      "metadata": {
        "id": "rZbL55jxkfiL"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = spark.read.text(\"/content/input_data/starter/transactions/d=2018-12-01/transactions.json\")\n",
        "a_df = a.select(F.from_json(a[\"value\"], transaction_schema).alias(\"map_cols\"))\n",
        "a_df.select(\n",
        "    \"map_cols.customer_id\",\n",
        "    \"map_cols.basket\",\n",
        "    \"map_cols.date_of_purchase\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GookMGTyywmS",
        "outputId": "ecf0ed83-28f1-49a3-9872-701cbdc07b58"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+-------------------+\n",
            "|customer_id|              basket|   date_of_purchase|\n",
            "+-----------+--------------------+-------------------+\n",
            "|         C1|        [{P29, 464}]|2018-12-01 04:11:00|\n",
            "|         C6|        [{P47, 279}]|2018-12-01 11:09:00|\n",
            "|        C10|        [{P55, 413}]|2018-12-01 19:38:00|\n",
            "|        C12|[{P62, 58}, {P62,...|2018-12-01 15:34:00|\n",
            "|        C13|[{P63, 340}, {P63...|2018-12-01 22:15:00|\n",
            "|        C22|        [{P43, 601}]|2018-12-01 05:36:00|\n",
            "|        C28|[{P11, 1839}, {P0...|2018-12-01 09:48:00|\n",
            "|        C30|[{P01, 1296}, {P0...|2018-12-01 23:20:00|\n",
            "|        C31|[{P43, 1375}, {P5...|2018-12-01 04:21:00|\n",
            "|        C39|[{P27, 1628}, {P2...|2018-12-01 21:35:00|\n",
            "|        C44|       [{P24, 1332}]|2018-12-01 22:59:00|\n",
            "|        C50|[{P26, 1848}, {P2...|2018-12-01 19:36:00|\n",
            "|        C55|[{P01, 1707}, {P0...|2018-12-01 13:46:00|\n",
            "|        C77|       [{P31, 1034}]|2018-12-01 07:23:00|\n",
            "|        C78|[{P30, 1740}, {P2...|2018-12-01 22:37:00|\n",
            "|        C79|[{P63, 129}, {P64...|2018-12-01 09:17:00|\n",
            "|       C103|[{P62, 160}, {P63...|2018-12-01 15:12:00|\n",
            "|       C109|[{P63, 896}, {P61...|2018-12-01 13:42:00|\n",
            "|       C118|[{P11, 333}, {P05...|2018-12-01 13:43:00|\n",
            "|       C130|        [{P40, 172}]|2018-12-01 15:46:00|\n",
            "+-----------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0fnienMI0N_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}